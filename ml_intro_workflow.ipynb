{"cells":[{"source":"![image](ml_workflow.png)\n","metadata":{},"id":"da1fd299-e1e2-44c1-9815-cd43262eefb0","cell_type":"markdown"},{"source":"## 1. Problem definition","metadata":{},"id":"eb30c699-c2f1-48e0-aad0-20bd286b582a","cell_type":"markdown"},{"source":"### 1.1 Types of Machine Learning Problems","metadata":{},"cell_type":"markdown","id":"f0ff89ab-9b2a-4fa3-b292-5c0393a90b16"},{"source":"There are three main types of machine learning problems:\n\n1. **Supervised Learning**: In this type of learning, the algorithm is trained on a labeled dataset, where the input data is accompanied by the correct output. The goal is to learn a mapping function that can predict the output for new input data.\n\n2. **Unsupervised Learning**: In this type of learning, the algorithm is trained on an unlabeled dataset, where the input data is not accompanied by the correct output. The goal is to learn the underlying structure or patterns in the data.\n\n3. **Reinforcement Learning**: In this type of learning, the algorithm learns by interacting with an environment. The goal is to learn a policy that maximizes a reward signal.\n\nThere is also a type of machine learning problem called **Transfer Learning**, which involves using knowledge gained from one task to improve performance on a different but related task. This can be useful when there is limited labeled data available for the target task.","metadata":{},"id":"490999de-6050-46a0-8800-4c08262b1603","cell_type":"markdown"},{"source":"### 1.2 Examples of Machine Learning Problems","metadata":{},"cell_type":"markdown","id":"cd992f0c-5f5c-4c0a-aa67-a43a07a45622"},{"source":"Here are some examples of machine learning problems:\n\n1. **Classification**: Predicting whether an email is spam or not.\n\n2. **Regression**: Predicting the price of a house based on its features.\n\n3. **Clustering**: Grouping customers based on their purchasing behavior.\n\n4. **Dimensionality Reduction**: Reducing the number of features in a dataset while preserving its structure.\n\n5. **Recommendation**: Recommending products or services to users based on their past behavior.\n\n6. **Natural Language Processing**: Analyzing and generating human language.\n\n7. **Computer Vision**: Analyzing and understanding visual data, such as images and videos.\n\n8. **Anomaly Detection**: Identifying unusual patterns or outliers in data.\n\nThese are just a few examples of the many types of machine learning problems that exist. Each problem requires a different approach and set of techniques to solve effectively.","metadata":{},"id":"6ba61526-cbc0-4276-9b77-5348319cadac","cell_type":"markdown"},{"source":"## 2. Data","metadata":{},"id":"f32610cf-11ba-4e73-a87c-dd806bc4515b","cell_type":"markdown"},{"source":"In machine learning, there are different types of data that can be used as input for algorithms. These data types include:\n\n- Numerical data: This type of data consists of numbers and can be either continuous or discrete. Examples include age, height, weight, temperature, etc.\n- Categorical data: This type of data consists of categories or labels that cannot be ordered or compared numerically. Examples include gender, race, occupation, etc.\n- Text data: This type of data consists of words or sentences and is used in natural language processing (NLP) tasks such as sentiment analysis, text classification, etc.\n- Image data: This type of data consists of pictures or images and is used in computer vision tasks such as object detection, image recognition, etc.\n- Audio data: This type of data consists of sound or speech and is used in speech recognition, speaker identification, etc.\n- Time-series data: This type of data consists of observations recorded over time, such as stock prices, weather data, etc. Time-series data requires special algorithms that can capture trends and patterns over time.","metadata":{},"id":"0f74fc14-91eb-4cf6-a199-683b535a1f6b","cell_type":"markdown"},{"source":"Streaming data is a type of data that is generated continuously over time and needs to be processed in real-time. Examples of streaming data include social media posts, sensor data, clickstream data, and financial data.\n\nStreaming data can be seen as a type of time-series data, but with some key differences. Time-series data is typically stored in a database or a file and analyzed offline, whereas streaming data is analyzed as it is being generated.","metadata":{},"id":"04536e49-62a9-453a-8f03-beb0260b3da8","cell_type":"markdown"},{"source":"## 3. Types of evaluation","metadata":{},"id":"28b85ad4-cfc8-459f-bc9b-eda4e863163e","cell_type":"markdown"},{"source":"In machine learning, metrics are used to measure the performance of a model on a given task. The choice of metric depends on the specific problem and the desired outcome. Here are some commonly used metrics in machine learning:\n\n- Accuracy: This is the most common metric used in classification problems. It measures the percentage of correctly classified instances out of all instances.\n- Precision: This metric measures the percentage of true positives (correctly predicted positive instances) out of all predicted positives.\n- Recall: This metric measures the percentage of true positives out of all actual positive instances.\n- F1 score: This is the harmonic mean of precision and recall, and is used to balance the trade-off between them.\n- Mean Squared Error (MSE): This metric is used in regression problems and measures the average squared difference between the predicted and actual values.\n- Mean Absolute Error (MAE): This metric is also used in regression problems and measures the average absolute difference between the predicted and actual values.\n- R-squared: This metric measures the proportion of variance in the target variable that is explained by the model. A higher R-squared value indicates a better fit of the model to the data.\n- Receiver Operating Characteristic (ROC) curve: This is a graphical representation of the trade-off between true positive rate and false positive rate for different classification thresholds.\n- Area Under the Curve (AUC): This metric is used to evaluate the performance of binary classification models based on the ROC curve. A higher AUC value indicates better performance.\n\nThe choice of metric depends on the specific problem and the desired outcome. For example, accuracy may be a good metric for a balanced dataset, while precision and recall may be more appropriate for imbalanced datasets. Similarly, R-squared may be more appropriate for regression problems, while AUC may be more appropriate for binary classification problems.","metadata":{},"id":"cc32bfc8-99b6-4697-9ab4-5ca34ad4794e","cell_type":"markdown"},{"source":"## 4. Features","metadata":{},"id":"b1516e72-691f-4751-aab6-c069f5901670","cell_type":"markdown"},{"source":"![features](features.png)\n","metadata":{},"id":"d50ed5ce-8f9a-4716-a62f-f4730e36a04f","cell_type":"markdown"},{"source":"In machine learning, features are the measurable properties or characteristics of the input data that are used to train a model. These features are used to represent the input data in a way that the machine learning algorithm can understand and make predictions on.\n\nFeatures can be categorized into three types:\n- Numeric features: These are features that are represented by numerical values. Examples include age, income, temperature, and height.\n- Categorical features: These are features that take on a limited number of possible values, such as color, gender, or country of origin.\n- Text features: These are features that are represented by text data, such as email messages or social media posts.","metadata":{},"id":"be43cd23-c410-4033-86db-e817426668d9","cell_type":"markdown"},{"source":"Features can also be transformed or engineered to create new features that may be more informative or relevant to the problem at hand. For example, in image recognition, features can be extracted from the raw pixel values, such as edges, corners, or texture patterns, to create higher-level features that capture more meaningful information.","metadata":{},"id":"2c4fd3aa-2ac8-46a9-b676-0c311d93400c","cell_type":"markdown"},{"source":"## 5. Modelling","metadata":{},"id":"5a221d24-360a-4aa0-8522-75b71da53ce6","cell_type":"markdown"},{"source":"![modelling](modelling.png)\n","metadata":{},"id":"4b6e4970-b9ed-47ac-a912-6091d2d354d3","cell_type":"markdown"},{"source":"![Uploading data_split.png](data_split.png)","metadata":{},"id":"0a1cffc6-1cf7-4636-8782-0e4015a55763","cell_type":"markdown"},{"source":"### 5.1 Picking the model","metadata":{},"id":"38e081ab-2875-4ffc-8b0b-46cc619668d9","cell_type":"markdown"},{"source":"![model](model.png)\n","metadata":{},"id":"db0ac129-9f01-440a-9d69-00627859b4fc","cell_type":"markdown"},{"source":"Choosing the right model is a critical step in the machine learning process and can greatly impact the performance of the system. Here are some important things to consider when choosing a model:\n\n**Problem type**: The type of problem you are trying to solve will dictate the type of model that is best suited for the task. For example, if you are working on a classification problem, you may want to consider using a decision tree, support vector machine (SVM), or logistic regression model. On the other hand, if you are working on a regression problem, you may want to consider using a linear regression, decision tree, or neural network model.\n\n**Dataset size**: The size of the dataset can also affect the choice of model. Some models work better with large datasets, while others may be more appropriate for smaller datasets. For example, deep learning models, such as convolutional neural networks (CNNs), may require large datasets to achieve good performance.\n\n**Complexity of the model**: More complex models may be able to capture more intricate patterns in the data, but they may also be more prone to overfitting, especially if the dataset is small. It's important to balance model complexity with model performance.\n\n**Interpretability**: Some models, such as decision trees, are more interpretable than others, such as deep neural networks. If interpretability is important for your application, you may want to consider using a model that is more transparent.\n\n**Scalability**: If you plan to deploy your model in a production environment, it's important to consider its scalability. Some models may be more efficient than others when it comes to processing large amounts of data or handling multiple requests simultaneously.\n\n**Resources**: The choice of model may also depend on the available resources, such as computing power or memory. For example, deep learning models may require powerful GPUs to train effectively.\n\n**Regularization**: Regularization techniques, such as L1 and L2 regularization, can help prevent overfitting and improve model generalization. Some models, such as linear regression, support regularization out of the box, while others may require additional tuning.","metadata":{},"id":"4f359769-b374-4ff9-823d-f90f6c0ba510","cell_type":"markdown"},{"source":"### 5.2 Tuning the model","metadata":{},"id":"8c093538-6ac9-441c-acb1-4eb3c9077d34","cell_type":"markdown"},{"source":"Model tuning, also known as **hyperparameter tuning**, is the process of finding the best combination of hyperparameters for a machine learning model to achieve optimal performance on a given task. Here are some steps you can follow to tune a model:\n\n**Define the hyperparameters**: Hyperparameters are parameters that are not learned during the training process, but instead are set by the user before training the model. These can include learning rate, regularization strength, batch size, and number of hidden layers in a neural network, among others.\n\n**Define a search space**: Once you have defined the hyperparameters, you need to define a search space for each hyperparameter. The search space can be continuous or discrete, and can include a range of values or a set of values to test.\n\n**Choose a search algorithm**: There are several algorithms you can use to search the hyperparameter space, including grid search, random search, and Bayesian optimization. Each algorithm has its own advantages and disadvantages, and the choice of algorithm will depend on the size of the search space and the resources available.\n\n**Train and evaluate the model**: For each combination of hyperparameters, train the model on a training set and evaluate its performance on a validation set. You can use metrics such as accuracy, precision, recall, or F1 score to evaluate the model.\n\n**Select the best model**: Once you have trained and evaluated the model for all combinations of hyperparameters, select the combination that gives the best performance on the validation set.\n\n**Test the model**: Finally, test the performance of the best model on a test set that was not used during the hyperparameter tuning process. This will give you an estimate of how well the model will perform on new, unseen data.\n\nIt's important to note that hyperparameter tuning can be a time-consuming and computationally expensive process, especially for deep learning models with many hyperparameters. However, by tuning the hyperparameters, you can significantly improve the performance of the model and achieve better results on your task.","metadata":{},"id":"846cc72e-48cf-46b7-9162-68c44f8ad5d4","cell_type":"markdown"},{"source":"### 5.3 Models comparison - testing the model","metadata":{},"id":"68b184f1-ca20-4c9b-a738-02e117ba4696","cell_type":"markdown"},{"source":"![model_testing](model_testing.png)","metadata":{},"id":"23a3466d-1ec9-499c-8ce7-9bfd5803a03b","cell_type":"markdown"},{"source":"![fitting](fitting.png)","metadata":{},"id":"cbeab01c-89b4-47a0-8d82-338cd25f7657","cell_type":"markdown"},{"source":"Overfitting occurs when a machine learning model fits the training data too closely and captures noise or random fluctuations in the data rather than the underlying pattern. This can lead to poor generalization performance on new, unseen data.\n\nThe main reasons for overfitting include:\n1. Insufficient training data: If the training data is too small or unrepresentative of the true distribution, the model may learn to fit the noise in the data instead of the underlying pattern. Collecting more data or using data augmentation techniques can help to mitigate this problem.\n2. Overly complex model: If the model is too complex, it may have too many parameters and be prone to overfitting. Simplifying the model architecture or adding regularization can help to prevent overfitting.\n3. Lack of regularization: Regularization techniques such as L1 or L2 regularization can help to prevent overfitting by adding a penalty term to the loss function that discourages large weights. Dropout regularization can also be used in neural networks to randomly drop out some of the neurons during training.\n4. Data leakage: If the validation set is contaminated with information from the training set, the model may overfit to the validation set and perform poorly on new data. It's important to ensure that the validation set is independent of the training set and that the data is properly shuffled and partitioned.\n5. Incorrect model selection: Choosing an inappropriate model for the task at hand can also lead to overfitting. It's important to choose a model that is not too complex and that can capture the underlying patterns in the data without overfitting.\n\nTo address overfitting, it's important to monitor the model's performance on both the training and validation sets during training and use appropriate regularization techniques to prevent overfitting. It's also important to ensure that the data is properly partitioned and that the model is chosen appropriately for the task at hand.","metadata":{},"id":"74493a8e-48de-4922-9907-504ff5cdfa51","cell_type":"markdown"},{"source":"Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and validation sets. The main reasons for underfitting include:\n1. Insufficient model complexity: If the model is too simple and has too few parameters, it may not be able to capture the complexity of the underlying patterns in the data. Increasing the model complexity by adding more layers or neurons, increasing the number of decision trees, or adding more features to the dataset can help to address underfitting.\n2. Insufficient training time: If the model is not trained for long enough, it may not have had enough time to learn the underlying patterns in the data. Increasing the number of training epochs or using a larger batch size can help to address this problem.\n3. Insufficient training data: If the training data is too small or unrepresentative of the true distribution, the model may not be able to learn the underlying patterns in the data. Collecting more data or using data augmentation techniques can help to mitigate this problem.\n4. Over-regularization: If the model is too heavily regularized, it may not be able to capture the underlying patterns in the data. Decreasing the strength of the regularization or using a different regularization technique can help to address this problem.\n5. Incorrect model selection: Choosing an inappropriate model for the task at hand can also lead to underfitting. It's important to choose a model that is complex enough to capture the underlying patterns in the data without overfitting.\n\nTo address underfitting, it's important to monitor the model's performance on both the training and validation sets during training and use appropriate regularization techniques to prevent over-regularization. It's also important to ensure that the model has sufficient complexity to capture the underlying patterns in the data and that the appropriate model is chosen for the task at hand.","metadata":{},"id":"c63c1783-56e0-410d-852b-f45b5fa23ae2","cell_type":"markdown"},{"source":"## 6. Experimentation","metadata":{},"id":"4cae5b14-1e00-4631-abd2-1e36ecd66e94","cell_type":"markdown"},{"source":"Experimentation in machine learning involves a cycle of designing, executing, and analyzing experiments to improve the performance of machine learning models. It is an iterative process that involves the steps 1-5.","metadata":{},"id":"e43704d3-3231-44ff-bb61-d92ff8f12766","cell_type":"markdown"},{"source":"## A1.  Tools","metadata":{},"id":"0e3fe6be-c538-43f3-a9ce-9be00b848092","cell_type":"markdown"},{"source":"![tools](tools.png)\n","metadata":{},"id":"a0f9a6b2-0951-48ca-9626-78a3979ae8e0","cell_type":"markdown"},{"source":"## A2. Credits","metadata":{},"cell_type":"markdown","id":"084b48af-7ba3-47e4-b7f0-19fe33b022fb"},{"source":"- Andrei Neagoie, Daniel Bourke - Complete Machine Learning & Data Science Bootcamp 2023\n- ChatGPT","metadata":{},"cell_type":"markdown","id":"9215a452-419f-4deb-b80b-173af1c4e57e"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}